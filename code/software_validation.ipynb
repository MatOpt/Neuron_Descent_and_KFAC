{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Implicit Fisher Computation\n",
    "\n",
    "The natural gradient algorithm used in the paper relies on implicitly inverting the Fisher and preconditioning gradients with it. To validate that these computations are correct, the results are compared to explictly computing natural gradients in small networks.\n",
    "\n",
    "The first cell below let's you decide which algorithm's correctness you want to verify. \n",
    "- For 'optimizer' you can choose 'natural' or 'natural_bd' (the latter being the block diagonal version).\n",
    "- For 'mc_fisher' you can choose 0 or 1. If 1, the fisher is approximated by sampling one label per input. If 0, the fisher is computed fully.\n",
    "- For network type, you can either choose 'fc' for a standard fully connected net, or 'conv' for a mix of convolutional and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'natural' # 'natural' or 'natural_bd'\n",
    "mc_fisher = 1 # 1 or 0\n",
    "network_type = 'fc' # 'fc' or 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version 1.8.1\n",
      "\n",
      "\n",
      "Using optimizer: natural\n",
      "Using MC Fisher: 1 \n",
      "\n",
      "\n",
      "Network: FOOFSequential(\n",
      "  (0): FOOFLinear(in_features=4, out_features=3, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): FOOFLinear(in_features=3, out_features=3, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): FOOFLinear(in_features=3, out_features=3, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "Explcitly computed natural gradient, first version:\n",
      " tensor([-0.2828, -0.0660, -0.4187,  0.0682,  0.0874, -0.3425,  0.0949,  0.0835,\n",
      "        -0.4336, -0.3140, -0.2616,  0.1682,  0.0000,  0.0388,  0.0000,  0.6727,\n",
      "         0.0000,  0.0916,  0.0521,  0.1551,  0.0755,  0.0497,  0.3377,  0.0411,\n",
      "        -0.1117, -0.0448, -0.0900,  0.0619, -0.2930,  0.0489])\n",
      "\n",
      "Explicitly computed natural gradient, second version\n",
      "(should be the same as first version):\n",
      " tensor([-0.2828, -0.0660, -0.4187,  0.0682,  0.0874, -0.3425,  0.0949,  0.0835,\n",
      "        -0.4336, -0.3140, -0.2616,  0.1682,  0.0000,  0.0388,  0.0000,  0.6727,\n",
      "         0.0000,  0.0916,  0.0521,  0.1551,  0.0755,  0.0497,  0.3377,  0.0411,\n",
      "        -0.1117, -0.0448, -0.0900,  0.0619, -0.2930,  0.0489])\n",
      "\n",
      "Implicity computed natural gradient. If this agrees with the explicit\n",
      "computation(s) above, the algorithm is implemented correctly.\n",
      " tensor([-0.2828, -0.0660, -0.4187,  0.0682,  0.0874, -0.3425,  0.0949,  0.0835,\n",
      "        -0.4336, -0.3140, -0.2616,  0.1682,  0.0000,  0.0388,  0.0000,  0.6727,\n",
      "         0.0000,  0.0916,  0.0521,  0.1551,  0.0755,  0.0497,  0.3377,  0.0411,\n",
      "        -0.1117, -0.0448, -0.0900,  0.0619, -0.2930,  0.0489])\n",
      "\n",
      "Squared norm of difference between implictly and explicitly computed updates\n",
      " 3.4705571749782393e-13\n",
      "Squared norms of implictly and explicitly computed update\n",
      " 1.5163168907165527 1.516316294670105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import architectures\n",
    "import optimization_modules as om\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cpu'\n",
    "print('Torch version', torch.__version__)\n",
    "\n",
    "print('\\n\\nUsing optimizer:', optimizer)\n",
    "print('Using MC Fisher:', mc_fisher, '\\n\\n')\n",
    "\n",
    "def brute_fisher_MC():\n",
    "    \"\"\"\n",
    "    This function explicitly computes the Fisher Information. \n",
    "    It uses global variables, notably a mini-batch 'X' and sampeld labels\n",
    "    'y_sampled' as well as a network 'net'.\n",
    "    \n",
    "    As in paper, the Fisher can be written as GGˆT. This function's output is\n",
    "    the triple GGˆT, GˆTG, G\n",
    "    \"\"\"\n",
    "    n_param = [0]\n",
    "    for par in net.parameters():\n",
    "        n_param.append(n_param[-1]+par.numel())\n",
    "    n_grads = 1 * n_data\n",
    "    \n",
    "    Fisher = torch.zeros(size=(n_param[-1],n_param[-1]))\n",
    "    G = torch.zeros(size=(n_param[-1],n_grads)).to(device)\n",
    "    curr_grad_id = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for i in range(X.size()[0]):\n",
    "        output = net(X[[i],:])\n",
    "        loss = criterion(output, y_sampled[i])\n",
    "        loss.backward()\n",
    "        G[:, curr_grad_id] = stacked_grad()\n",
    "        curr_grad_id += 1\n",
    "        net.zero_grad()\n",
    "    G /= torch.sqrt(torch.tensor(n_data))\n",
    "    Fisher = torch.mm(G, G.t()) \n",
    "    Gram = torch.mm(G.t(), G) \n",
    "    return Fisher, Gram, G\n",
    "\n",
    "def brute_fisher_full():\n",
    "    \"\"\"\n",
    "    Analogous to brute_fisher_mc, but computes full fisher \n",
    "    rather than an MC sample.\n",
    "    \"\"\"\n",
    "    n_param = [0]\n",
    "    for par in net.parameters():\n",
    "        n_param.append(n_param[-1]+par.numel())\n",
    "    n_grads = output_dim * n_data\n",
    "    \n",
    "    Fisher = torch.zeros(size=(n_param[-1],n_param[-1]))\n",
    "    G = torch.zeros(size=(n_param[-1],n_grads)).to(device)\n",
    "    curr_grad_id = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for i in range(X.size()[0]):\n",
    "        for label_id in range(len(label_list)):\n",
    "            output = net(X[[i],:])\n",
    "            label = label_list[[label_id]]\n",
    "            probs = torch.nn.functional.softmax(output,dim=1).detach()\n",
    "            loss = criterion(output, label) * torch.sqrt(probs[0][label_id])\n",
    "            loss.backward()\n",
    "            G[:, curr_grad_id] = stacked_grad()\n",
    "            curr_grad_id += 1\n",
    "            net.zero_grad()\n",
    "    G /= torch.sqrt(torch.tensor(n_data))\n",
    "    Fisher = torch.mm(G, G.t()) \n",
    "    Gram = torch.mm(G.t(), G) \n",
    "    return Fisher, Gram, G\n",
    "\n",
    "def block_diagonal(Fisher):\n",
    "    \"\"\"\n",
    "    Thus function replaces the entries of the input matrix, which \n",
    "    are not on the block-diagonal by 0.\n",
    "    \"\"\"\n",
    "    n_param = [0]\n",
    "    for par in net.parameters():\n",
    "        #only non-bias parameters\n",
    "        if len(par.size()) > 0:\n",
    "            n_param.append(n_param[-1]+par.numel())\n",
    "    mask = torch.zeros(size=(n_param[-1],n_param[-1]))\n",
    "    for n0, n1 in zip(n_param[:-1], n_param[1:]):\n",
    "        mask[n0:n1, :][:, n0:n1] = 1.\n",
    "    return Fisher*mask\n",
    "\n",
    "def stacked_grad():\n",
    "    \"\"\"\n",
    "    Returns the gradient of the network as a 1-dimensional vector.\n",
    "    \"\"\"\n",
    "    stacked_grad = torch.tensor([])#torch.zeros(size=(n_param[-1],1))\n",
    "    for layer in net.my_modules():\n",
    "        stacked_grad = torch.cat((stacked_grad, layer.weight.grad.reshape(layer.weight.numel())))\n",
    "    return stacked_grad\n",
    "\n",
    "def stacked_update():\n",
    "    \"\"\"\n",
    "    Returns the update_direction of the network as a 1-dimensional vector.\n",
    "    [This relies on using the FOOFSequential class defined in optimization_modules]\n",
    "    \"\"\"\n",
    "    stacked_grad = torch.tensor([])#torch.zeros(size=(n_param[-1],1))\n",
    "    for layer in net.my_modules():\n",
    "        #stacked_grad = torch.cat((stacked_grad, layer.up_dir_weight.reshape(layer.weight.numel())))\n",
    "        stacked_grad = torch.cat((stacked_grad, layer.update_direction.reshape(layer.weight.numel())))\n",
    "    return stacked_grad\n",
    "\n",
    "# Specify (synthetic) data\n",
    "input_dim = 4\n",
    "output_dim = 3\n",
    "n_data = 7\n",
    "n_data = int(n_data)\n",
    "if network_type == 'fc':\n",
    "    X = torch.normal(0,1,size=(n_data, input_dim)).to(device)\n",
    "if network_type == 'conv':\n",
    "    X = torch.normal(0,1,size=(n_data, 1, input_dim, input_dim)).to(device)\n",
    "y_target = torch.randint(low=0, high=output_dim, size=(n_data,1)).long()\n",
    "label_list = torch.tensor(range(output_dim)).long()\n",
    "\n",
    "# Specify damping term for Fisher\n",
    "lam = 0.3\n",
    "\n",
    "# Specify the network\n",
    "if network_type == 'fc': \n",
    "    width = 3\n",
    "    depth = 2\n",
    "    net = architectures.SimpleFCNet(width=width,\n",
    "                         depth=depth,\n",
    "                         input_dim=input_dim,\n",
    "                         output_dim=output_dim,\n",
    "                         lr=0.0,\n",
    "                         damp=lam, \n",
    "                         optimizer=None,  #optimizer will be set later\n",
    "                         mc_fisher=mc_fisher)\n",
    "    \n",
    "if network_type == 'conv':\n",
    "    # Create a simple conv net from scratch\n",
    "    module_list = []\n",
    "    module_list += [om.FOOFConv2d(in_channels=1,out_channels=2,\n",
    "                                  kernel_size=3,padding=1,bias=False)]\n",
    "    module_list += [torch.nn.ReLU()]\n",
    "    module_list += [om.FOOFConv2d(in_channels=2,out_channels=2,\n",
    "                                  kernel_size=3,padding=1,bias=False)]\n",
    "    module_list += [torch.nn.ReLU()]\n",
    "    module_list += [torch.nn.AvgPool2d(kernel_size=2)]\n",
    "    module_list += [torch.nn.Flatten()]\n",
    "    module_list += [om.FOOFLinear(n_in_features=2*input_dim**2//4, \n",
    "                                  n_out_features=output_dim, bias=False)]\n",
    "    net = om.FOOFSequential(module_list, \n",
    "                            lr=0.0,\n",
    "                            damp=lam, \n",
    "                            optimizer=None,  #optimizer will be set later\n",
    "                            mc_fisher=mc_fisher,\n",
    "                            output_dim=output_dim)\n",
    "\n",
    "print('Network:', net)\n",
    "net.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Sample a set of labels. Note that for the purpose of this validation, \n",
    "# the labels do not need to be sampled from the model distribution\n",
    "# This variable will only be used if mc_fisher==0.\n",
    "y_sampled = torch.randint(low=0, high=output_dim, size=(n_data,1)).long()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute Fisher Explicitly\n",
    "if mc_fisher:\n",
    "    Fisher, Gram, G = brute_fisher_MC()\n",
    "else:\n",
    "    Fisher, Gram, G = brute_fisher_full()\n",
    "# And make block diagonal, if needed. \n",
    "if optimizer == 'natural_bd':\n",
    "    Fisher = block_diagonal(Fisher)\n",
    "\n",
    "# Comptue Gradients Explciitly\n",
    "output = net(X)\n",
    "loss = criterion(output, y_target[:,0])\n",
    "loss.backward()\n",
    "grad = stacked_grad()\n",
    "net.zero_grad()\n",
    "\n",
    "# Compute the natural gradient update explicitly\n",
    "natural_update_brute_1 = torch.mv(torch.inverse(lam*torch.eye(Fisher.shape[0]) + Fisher), \n",
    "                                  grad)\n",
    "\n",
    "# Compute the natural gradient update explicitly, in a differnt way (relying on the matrix inversion lemma)\n",
    "A = 1/lam * torch.eye(Fisher.shape[0]) \\\n",
    "    - (1/lam**2)* G @ torch.inverse(torch.eye(Gram.shape[0])+1/lam*Gram) @ G.t()\n",
    "natural_update_brute_2 = torch.mv(A, grad)\n",
    "\n",
    "print('\\n\\nExplcitly computed natural gradient, first version:\\n', \n",
    "      natural_update_brute_1)\n",
    "if optimizer == 'natural':\n",
    "    print('\\nExplicitly computed natural gradient, second version\\n\\\n",
    "(should be the same as first version):\\n', \n",
    "     natural_update_brute_2)\n",
    "\n",
    "# Now compute the natural update implicitly, using method described in paper.\n",
    "net.set_optimizer(optimizer)\n",
    "# First carry out implicit Fisher computations. \n",
    "# Note that we need to use the same sampled labels as before. \n",
    "# Note that y_sampled is ignored, if mc_fisher==0.\n",
    "net.update_fisher(X, y=y_sampled[:,0])\n",
    "# Now compute the parameter update\n",
    "net.parameter_update(X, y_target[:,0])\n",
    "update_implicit = stacked_update()\n",
    "\n",
    "print('\\nImplicity computed natural gradient. If this agrees with the explicit\\ncomputation(s)\\\n",
    " above, the algorithm is implemented correctly.\\n', \n",
    " update_implicit)\n",
    "\n",
    "print('\\nSquared norm of difference between implictly and explicitly computed updates\\n', \n",
    "      (update_implicit-natural_update_brute_1).pow(2).sum().item())\n",
    "print('Squared norms of implictly and explicitly computed update\\n', \n",
    "      (update_implicit).pow(2).sum().item(), (natural_update_brute_1).pow(2).sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayfac",
   "language": "python",
   "name": "bayfac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
